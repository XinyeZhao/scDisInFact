{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Iterable, List\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD\n",
    "def compute_pairwise_distances(x, y):\n",
    "    '''\n",
    "    ||x - y||^2 = x^2 + y^2 - 2xy\n",
    "    '''\n",
    "    # * .view(-1, 1) -> changee to col vector\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y_t = y.t()\n",
    "    y_norm = (y**2).sum(1).view(1, -1)\n",
    "    # * torch.mm -> matrix muptilply\n",
    "    # * .sum(1) -> sum by row\n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    # * torch.clamp -> change input value to be between 0.0 and np.inf\n",
    "    # return torch.clamp(dist, 0.0, np.inf)\n",
    "    return torch.abs(dist)\n",
    "\n",
    "def _gaussian_kernel_matrix(x, y):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    sigmas = torch.FloatTensor([1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100, 1e3, 1e4, 1e5, 1e6]).to(device)\n",
    "    dist = compute_pairwise_distances(x, y)\n",
    "    beta = 1. / (2. * sigmas.view(-1, 1))\n",
    "    s = - beta.mm(dist.reshape((1, -1)) )\n",
    "    result = torch.sum(torch.exp(s), dim = 0)\n",
    "    return result\n",
    "\n",
    "def maximum_mean_discrepancy(x, y): #Function to calculate MMD value\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    cost_xx = torch.mean(_gaussian_kernel_matrix(x, x))\n",
    "    cost_yy = torch.mean(_gaussian_kernel_matrix(y, y))\n",
    "    cost_xy = 2.0 * torch.mean(_gaussian_kernel_matrix(x, y))\n",
    "    cost = torch.sqrt((cost_xx + cost_yy - cost_xy) ** 2 + 1e-9)\n",
    "    if cost.data.item()<0:\n",
    "        cost = torch.FloatTensor([0.0]).to(device)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, features = [1000, 500, 500], use_batch_norm = True, dropout_rate = 0.0, negative_slope = 0.0, use_bias = True):\n",
    "        super(FC, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.fc_layers = []\n",
    "\n",
    "        # create fc layers according to the layers_dim\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"Layer {}\".format(i),\n",
    "                        nn.Sequential(\n",
    "                            collections.OrderedDict(\n",
    "                                [\n",
    "                                    (\"linear\", nn.Linear(n_in, n_out) if use_bias else nn.Linear(n_in, n_out, bias = False),),\n",
    "                                    (\"batchnorm\", nn.BatchNorm1d(n_out, momentum=0.01, eps=0.001) if use_batch_norm else None,),\n",
    "                                    (\"relu\", nn.ReLU() if negative_slope <= 0 else nn.LeakyReLU(negative_slope = negative_slope),),\n",
    "                                    (\"dropout\", nn.Dropout(p=dropout_rate) if dropout_rate > 0 else None,),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    )\n",
    "                    for i, (n_in, n_out) in enumerate(zip(self.features[:-1], self.features[1:]))\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # loop through all layers\n",
    "        for layers in self.fc_layers:\n",
    "            # loop through linear, batchnorm, relu, dropout, etc\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FC(\n",
       "  (fc_layers): Sequential(\n",
       "    (Layer 0): Sequential(\n",
       "      (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (batchnorm): BatchNorm1d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): None\n",
       "    )\n",
       "    (Layer 1): Sequential(\n",
       "      (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (batchnorm): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): None\n",
       "    )\n",
       "    (Layer 2): Sequential(\n",
       "      (linear): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (batchnorm): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): None\n",
       "    )\n",
       "    (Layer 3): Sequential(\n",
       "      (linear): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (batchnorm): BatchNorm1d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): None\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = FC(features= [1024, 512, 128, 32, 8])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features = [1024, 256, 32, 8], dropout_rate = 0.1, negative_slope = 0.2):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        if len(features) > 2:\n",
    "            self.fc = FC(\n",
    "                features = features[:-1],\n",
    "                dropout_rate = dropout_rate,\n",
    "                negative_slope = negative_slope,\n",
    "                use_bias = True\n",
    "            )\n",
    "        self.output = nn.Linear(features[-2], features[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(self.features) > 2:\n",
    "            x = self.fc(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features = [8, 32, 256, 1024], dropout_rate = 0.0, negative_slope = 0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = FC(\n",
    "            features = features,\n",
    "            dropout_rate = dropout_rate,\n",
    "            negative_slope = negative_slope,\n",
    "            use_bias = True\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # The decoder returns values for the parameters of the ZINB distribution\n",
    "        x_mean = self.fc(z)\n",
    "        return x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gene_act(nn.Module):\n",
    "    def __init__(self, features = [1000, 500, 500], use_batch_norm = True, dropout_rate = 0.0, negative_slope = 0.0):\n",
    "        super(gene_act, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.fc_layers = []\n",
    "\n",
    "        # create fc layers according to the layers_dim\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"Layer {}\".format(i),\n",
    "                        nn.Sequential(\n",
    "                            collections.OrderedDict(\n",
    "                                [\n",
    "                                    (\"linear\", nn.Linear(n_in, n_out, bias = False),),\n",
    "                                    (\"batchnorm\", nn.BatchNorm1d(n_out, momentum=0.01, eps=0.001) if use_batch_norm else None,),\n",
    "                                    (\"act\", nn.ReLU() if negative_slope <= 0 else nn.LeakyReLU(negative_slope = negative_slope),),\n",
    "                                    (\"dropout\", nn.Dropout(p=dropout_rate) if dropout_rate > 0 else None,),\n",
    "                                ]\n",
    "                            )\n",
    "                        ),\n",
    "                    )\n",
    "                    for i, (n_in, n_out) in enumerate(zip(self.features[:-1], self.features[1:]))\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # loop through all layers\n",
    "        for layers in self.fc_layers:\n",
    "            # loop through linear, batchnorm, relu, dropout, etc\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    x = layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mtx = r\"D:\\Francis Secrets\\Pycharm Projects\\xsede\\dataset284\\covid284_h5file\\matrix_by_sample\\mtx_0.mtx\"\n",
    "data_test = sc.read_mtx(path_mtx)\n",
    "mtx_data = np.array(data_test.X.todense())\n",
    "# mtx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27943, 238)\n"
     ]
    }
   ],
   "source": [
    "print(mtx_data.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(200, 27943)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001410507CAC0>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "all_data = mtx_data\n",
    "random.seed(214)\n",
    "m = len(all_data[:200])\n",
    "print(m)\n",
    "# print([int(m - m * 0.2), int(m * 0.2)])\n",
    "train_data, val_data = random_split(dataset=all_data[:200], lengths=[int(m - m * 0.2), int(m * 0.2)])\n",
    "# train_data, val_data = all_data[:150], all_data[151:200]\n",
    "test_data = all_data[201:]\n",
    "print((train_data.dataset.shape))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 27943])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'dataset'\n",
    "# train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "# test_dataset = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "# train_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "# test_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "# train_dataset.transform = train_transform\n",
    "# test_dataset.transform = test_transform\n",
    "\n",
    "# m = len(train_dataset)\n",
    "\n",
    "# train_data, val_data = random_split(mtx_data, [int(m-m*0.2), int(m*0.2)])\n",
    "# batch_size = 512\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "# valid_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(features=[train_data.shape[1], 1024, 256, 64, 16 ])\n",
    "\n",
    "# decoder = Decoder(features=[train_data.shape[1], 1024, 256, 64, 16 ][::-1])\n",
    "encoder = Encoder(features=[all_data.shape[1], 1024, 256, 64, 16])\n",
    "decoder = Decoder(features=[all_data.shape[1], 1024, 256, 64, 16][::-1])\n",
    "# Define loss func\n",
    "class MMD_LOSS(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, output, target):\n",
    "        loss_MSE = nn.MSELoss().forward(output, target)\n",
    "        loss_MMD = maximum_mean_discrepancy(output, target)\n",
    "        return loss_MMD + loss_MSE\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define Optimizer\n",
    "lr = 0.001\n",
    "\n",
    "#Random seed\n",
    "torch.manual_seed(0)\n",
    "param_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(param_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "def add_noise(inputs, noise_factor=0.3):\n",
    "    noisy = inputs + torch.randn_like(inputs) * noise_factor\n",
    "    noisy = torch.clip(noisy, 0., 1.)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor(16.6291)\n"
     ]
    }
   ],
   "source": [
    "ls = MMD_LOSS()\n",
    "# ls.forward()\n",
    "# print(ls([1,2,3], [1,2,3]))\n",
    "a = torch.zeros([2, 4])\n",
    "b = torch.ones([2,4])\n",
    "print(a, b)\n",
    "# ls.forward(a, b)\n",
    "print(ls.forward(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc): FC(\n",
       "    (fc_layers): Sequential(\n",
       "      (Layer 0): Sequential(\n",
       "        (linear): Linear(in_features=16, out_features=64, bias=True)\n",
       "        (batchnorm): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.2)\n",
       "        (dropout): None\n",
       "      )\n",
       "      (Layer 1): Sequential(\n",
       "        (linear): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (batchnorm): BatchNorm1d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.2)\n",
       "        (dropout): None\n",
       "      )\n",
       "      (Layer 2): Sequential(\n",
       "        (linear): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (batchnorm): BatchNorm1d(1024, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.2)\n",
       "        (dropout): None\n",
       "      )\n",
       "      (Layer 3): Sequential(\n",
       "        (linear): Linear(in_features=1024, out_features=27943, bias=True)\n",
       "        (batchnorm): BatchNorm1d(27943, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.2)\n",
       "        (dropout): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_epoch(encoder, decoder, dataloader, loss_fn, optimizer, noise_factor=0.3):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    for image_batch in dataloader:\n",
    "        # _\n",
    "        # print(image_batch)\n",
    "        image_noisy = add_noise(image_batch, noise_factor)\n",
    "        # print(image_noisy.shape)\n",
    "        # Encode\n",
    "        # image_noisy_1 = image_noisy.view(-1, 1)\n",
    "        encoded_data = encoder(image_noisy)\n",
    "        # Decode\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_noisy)\n",
    "        # Backword \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print\n",
    "        print('\\t partial train loss : %f' %(loss.data))\n",
    "        train_loss.append(loss.detach().numpy())\n",
    "        return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing func\n",
    "def test_epoch(encoder, decoder, dataloader, loss_fn, noise_factor=0.3):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # Don't track gradients\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch in dataloader:\n",
    "            image_noisy = add_noise(image_batch, noise_factor)\n",
    "            # Encode\n",
    "            encoded_data = encoder(image_noisy)\n",
    "            # Decode\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the output list to the original image\n",
    "            conc_out.append(decoded_data)\n",
    "            conc_label.append(image_batch)\n",
    "            # Create single tensor with all values\n",
    "            conc_out = torch.cat(conc_out)\n",
    "            conc_label = torch.cat(conc_label)\n",
    "            # Evaluate global loss\n",
    "            val_loss = loss_fn(conc_out, conc_label)\n",
    "            return val_loss.data            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_ae_outputs(encoder,decoder,n=5):\n",
    "#     plt.figure(figsize=(10,4.5))\n",
    "#     for i in range(n):\n",
    "#       ax = plt.subplot(2,n,i+1)\n",
    "#       img = test_dataset[i][0].unsqueeze(0)\n",
    "#       encoder.eval()\n",
    "#       decoder.eval()\n",
    "#       with torch.no_grad():\n",
    "#          rec_img  = decoder(encoder(img))\n",
    "#       plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "#       ax.get_xaxis().set_visible(False)\n",
    "#       ax.get_yaxis().set_visible(False)  \n",
    "#       if i == n//2:\n",
    "#         ax.set_title('Original images')\n",
    "#       ax = plt.subplot(2, n, i + 1 + n)\n",
    "#       plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "#       ax.get_xaxis().set_visible(False)\n",
    "#       ax.get_yaxis().set_visible(False)  \n",
    "#       if i == n//2:\n",
    "#          ax.set_title('Reconstructed images')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss : 0.350189\n",
      "\n",
      " EPOCH 1/100 \t train loss 0.3501893877983093 \t val loss 0.16649560630321503\n",
      "\t partial train loss : 0.249231\n",
      "\n",
      " EPOCH 2/100 \t train loss 0.24923096597194672 \t val loss 0.16706979274749756\n",
      "\t partial train loss : 0.192376\n",
      "\n",
      " EPOCH 3/100 \t train loss 0.1923755407333374 \t val loss 0.16756689548492432\n",
      "\t partial train loss : 0.167297\n",
      "\n",
      " EPOCH 4/100 \t train loss 0.16729727387428284 \t val loss 0.1680460274219513\n",
      "\t partial train loss : 0.155158\n",
      "\n",
      " EPOCH 5/100 \t train loss 0.15515762567520142 \t val loss 0.16850772500038147\n",
      "\t partial train loss : 0.149020\n",
      "\n",
      " EPOCH 6/100 \t train loss 0.14901991188526154 \t val loss 0.1692381650209427\n",
      "\t partial train loss : 0.144849\n",
      "\n",
      " EPOCH 7/100 \t train loss 0.14484873414039612 \t val loss 0.1698703020811081\n",
      "\t partial train loss : 0.142631\n",
      "\n",
      " EPOCH 8/100 \t train loss 0.1426311880350113 \t val loss 0.17035675048828125\n",
      "\t partial train loss : 0.140654\n",
      "\n",
      " EPOCH 9/100 \t train loss 0.14065386354923248 \t val loss 0.17146095633506775\n",
      "\t partial train loss : 0.139427\n",
      "\n",
      " EPOCH 10/100 \t train loss 0.13942667841911316 \t val loss 0.17168684303760529\n",
      "\t partial train loss : 0.138078\n",
      "\n",
      " EPOCH 11/100 \t train loss 0.1380784660577774 \t val loss 0.1723335236310959\n",
      "\t partial train loss : 0.136897\n",
      "\n",
      " EPOCH 12/100 \t train loss 0.1368969827890396 \t val loss 0.17290689051151276\n",
      "\t partial train loss : 0.136101\n",
      "\n",
      " EPOCH 13/100 \t train loss 0.13610094785690308 \t val loss 0.17219315469264984\n",
      "\t partial train loss : 0.135079\n",
      "\n",
      " EPOCH 14/100 \t train loss 0.1350792646408081 \t val loss 0.17189238965511322\n",
      "\t partial train loss : 0.133747\n",
      "\n",
      " EPOCH 15/100 \t train loss 0.13374748826026917 \t val loss 0.1702905297279358\n",
      "\t partial train loss : 0.133266\n",
      "\n",
      " EPOCH 16/100 \t train loss 0.13326631486415863 \t val loss 0.1701590120792389\n",
      "\t partial train loss : 0.132815\n",
      "\n",
      " EPOCH 17/100 \t train loss 0.13281533122062683 \t val loss 0.16859027743339539\n",
      "\t partial train loss : 0.131344\n",
      "\n",
      " EPOCH 18/100 \t train loss 0.13134431838989258 \t val loss 0.16745787858963013\n",
      "\t partial train loss : 0.131299\n",
      "\n",
      " EPOCH 19/100 \t train loss 0.13129931688308716 \t val loss 0.16616712510585785\n",
      "\t partial train loss : 0.130377\n",
      "\n",
      " EPOCH 20/100 \t train loss 0.1303768754005432 \t val loss 0.16564138233661652\n",
      "\t partial train loss : 0.129773\n",
      "\n",
      " EPOCH 21/100 \t train loss 0.12977346777915955 \t val loss 0.16346041858196259\n",
      "\t partial train loss : 0.129028\n",
      "\n",
      " EPOCH 22/100 \t train loss 0.12902754545211792 \t val loss 0.16299232840538025\n",
      "\t partial train loss : 0.128682\n",
      "\n",
      " EPOCH 23/100 \t train loss 0.12868206202983856 \t val loss 0.16274112462997437\n",
      "\t partial train loss : 0.127979\n",
      "\n",
      " EPOCH 24/100 \t train loss 0.12797868251800537 \t val loss 0.16071441769599915\n",
      "\t partial train loss : 0.127459\n",
      "\n",
      " EPOCH 25/100 \t train loss 0.12745878100395203 \t val loss 0.16049838066101074\n",
      "\t partial train loss : 0.127276\n",
      "\n",
      " EPOCH 26/100 \t train loss 0.127275750041008 \t val loss 0.1596454530954361\n",
      "\t partial train loss : 0.126722\n",
      "\n",
      " EPOCH 27/100 \t train loss 0.1267216056585312 \t val loss 0.15864954888820648\n",
      "\t partial train loss : 0.125798\n",
      "\n",
      " EPOCH 28/100 \t train loss 0.12579843401908875 \t val loss 0.15792794525623322\n",
      "\t partial train loss : 0.125497\n",
      "\n",
      " EPOCH 29/100 \t train loss 0.1254972368478775 \t val loss 0.1574859321117401\n",
      "\t partial train loss : 0.124934\n",
      "\n",
      " EPOCH 30/100 \t train loss 0.12493441998958588 \t val loss 0.15697705745697021\n",
      "\t partial train loss : 0.124747\n",
      "\n",
      " EPOCH 31/100 \t train loss 0.1247466430068016 \t val loss 0.15653982758522034\n",
      "\t partial train loss : 0.124360\n",
      "\n",
      " EPOCH 32/100 \t train loss 0.12435983121395111 \t val loss 0.15624533593654633\n",
      "\t partial train loss : 0.123614\n",
      "\n",
      " EPOCH 33/100 \t train loss 0.12361402809619904 \t val loss 0.15531904995441437\n",
      "\t partial train loss : 0.123162\n",
      "\n",
      " EPOCH 34/100 \t train loss 0.12316246330738068 \t val loss 0.15501335263252258\n",
      "\t partial train loss : 0.123007\n",
      "\n",
      " EPOCH 35/100 \t train loss 0.12300704419612885 \t val loss 0.15476767718791962\n",
      "\t partial train loss : 0.122423\n",
      "\n",
      " EPOCH 36/100 \t train loss 0.12242287397384644 \t val loss 0.15354864299297333\n",
      "\t partial train loss : 0.122205\n",
      "\n",
      " EPOCH 37/100 \t train loss 0.12220512330532074 \t val loss 0.15363526344299316\n",
      "\t partial train loss : 0.121717\n",
      "\n",
      " EPOCH 38/100 \t train loss 0.12171719968318939 \t val loss 0.153193861246109\n",
      "\t partial train loss : 0.121393\n",
      "\n",
      " EPOCH 39/100 \t train loss 0.1213926300406456 \t val loss 0.15325942635536194\n",
      "\t partial train loss : 0.120862\n",
      "\n",
      " EPOCH 40/100 \t train loss 0.12086223810911179 \t val loss 0.15161527693271637\n",
      "\t partial train loss : 0.120622\n",
      "\n",
      " EPOCH 41/100 \t train loss 0.12062247097492218 \t val loss 0.15101011097431183\n",
      "\t partial train loss : 0.120088\n",
      "\n",
      " EPOCH 42/100 \t train loss 0.12008833885192871 \t val loss 0.15060213208198547\n",
      "\t partial train loss : 0.119851\n",
      "\n",
      " EPOCH 43/100 \t train loss 0.11985132098197937 \t val loss 0.1504088044166565\n",
      "\t partial train loss : 0.119527\n",
      "\n",
      " EPOCH 44/100 \t train loss 0.11952728778123856 \t val loss 0.14923539757728577\n",
      "\t partial train loss : 0.119145\n",
      "\n",
      " EPOCH 45/100 \t train loss 0.11914527416229248 \t val loss 0.14944691956043243\n",
      "\t partial train loss : 0.118977\n",
      "\n",
      " EPOCH 46/100 \t train loss 0.11897725611925125 \t val loss 0.14879944920539856\n",
      "\t partial train loss : 0.118388\n",
      "\n",
      " EPOCH 47/100 \t train loss 0.11838816851377487 \t val loss 0.1475186049938202\n",
      "\t partial train loss : 0.118078\n",
      "\n",
      " EPOCH 48/100 \t train loss 0.11807794868946075 \t val loss 0.14734923839569092\n",
      "\t partial train loss : 0.117576\n",
      "\n",
      " EPOCH 49/100 \t train loss 0.11757614463567734 \t val loss 0.14638377726078033\n",
      "\t partial train loss : 0.117269\n",
      "\n",
      " EPOCH 50/100 \t train loss 0.11726859956979752 \t val loss 0.1464834213256836\n",
      "\t partial train loss : 0.117296\n",
      "\n",
      " EPOCH 51/100 \t train loss 0.1172957494854927 \t val loss 0.144583597779274\n",
      "\t partial train loss : 0.117365\n",
      "\n",
      " EPOCH 52/100 \t train loss 0.11736541986465454 \t val loss 0.14466440677642822\n",
      "\t partial train loss : 0.116541\n",
      "\n",
      " EPOCH 53/100 \t train loss 0.1165408343076706 \t val loss 0.14414438605308533\n",
      "\t partial train loss : 0.116192\n",
      "\n",
      " EPOCH 54/100 \t train loss 0.11619243025779724 \t val loss 0.14349432289600372\n",
      "\t partial train loss : 0.116032\n",
      "\n",
      " EPOCH 55/100 \t train loss 0.11603161692619324 \t val loss 0.14319373667240143\n",
      "\t partial train loss : 0.115540\n",
      "\n",
      " EPOCH 56/100 \t train loss 0.11553999781608582 \t val loss 0.14228026568889618\n",
      "\t partial train loss : 0.115277\n",
      "\n",
      " EPOCH 57/100 \t train loss 0.11527678370475769 \t val loss 0.1419437974691391\n",
      "\t partial train loss : 0.114994\n",
      "\n",
      " EPOCH 58/100 \t train loss 0.11499439924955368 \t val loss 0.14180490374565125\n",
      "\t partial train loss : 0.114790\n",
      "\n",
      " EPOCH 59/100 \t train loss 0.11479020118713379 \t val loss 0.14125750958919525\n",
      "\t partial train loss : 0.114474\n",
      "\n",
      " EPOCH 60/100 \t train loss 0.1144736260175705 \t val loss 0.14079637825489044\n",
      "\t partial train loss : 0.114315\n",
      "\n",
      " EPOCH 61/100 \t train loss 0.11431510001420975 \t val loss 0.14041966199874878\n",
      "\t partial train loss : 0.113895\n",
      "\n",
      " EPOCH 62/100 \t train loss 0.11389518529176712 \t val loss 0.1399770826101303\n",
      "\t partial train loss : 0.113655\n",
      "\n",
      " EPOCH 63/100 \t train loss 0.11365462094545364 \t val loss 0.1396072953939438\n",
      "\t partial train loss : 0.113463\n",
      "\n",
      " EPOCH 64/100 \t train loss 0.11346311122179031 \t val loss 0.13923873007297516\n",
      "\t partial train loss : 0.113219\n",
      "\n",
      " EPOCH 65/100 \t train loss 0.1132190153002739 \t val loss 0.13880041241645813\n",
      "\t partial train loss : 0.112797\n",
      "\n",
      " EPOCH 66/100 \t train loss 0.11279679834842682 \t val loss 0.1388818621635437\n",
      "\t partial train loss : 0.112504\n",
      "\n",
      " EPOCH 67/100 \t train loss 0.11250429600477219 \t val loss 0.13835947215557098\n",
      "\t partial train loss : 0.112469\n",
      "\n",
      " EPOCH 68/100 \t train loss 0.1124686598777771 \t val loss 0.13788250088691711\n",
      "\t partial train loss : 0.112210\n",
      "\n",
      " EPOCH 69/100 \t train loss 0.11221003532409668 \t val loss 0.1381535828113556\n",
      "\t partial train loss : 0.111941\n",
      "\n",
      " EPOCH 70/100 \t train loss 0.11194057762622833 \t val loss 0.13745971024036407\n",
      "\t partial train loss : 0.111817\n",
      "\n",
      " EPOCH 71/100 \t train loss 0.1118169054389 \t val loss 0.13711071014404297\n",
      "\t partial train loss : 0.111580\n",
      "\n",
      " EPOCH 72/100 \t train loss 0.11157964915037155 \t val loss 0.1371692270040512\n",
      "\t partial train loss : 0.111100\n",
      "\n",
      " EPOCH 73/100 \t train loss 0.1110997349023819 \t val loss 0.13677532970905304\n",
      "\t partial train loss : 0.110915\n",
      "\n",
      " EPOCH 74/100 \t train loss 0.11091486364603043 \t val loss 0.13654334843158722\n",
      "\t partial train loss : 0.110569\n",
      "\n",
      " EPOCH 75/100 \t train loss 0.11056927591562271 \t val loss 0.1365772783756256\n",
      "\t partial train loss : 0.110304\n",
      "\n",
      " EPOCH 76/100 \t train loss 0.11030422151088715 \t val loss 0.13633666932582855\n",
      "\t partial train loss : 0.110248\n",
      "\n",
      " EPOCH 77/100 \t train loss 0.11024826020002365 \t val loss 0.13595110177993774\n",
      "\t partial train loss : 0.109844\n",
      "\n",
      " EPOCH 78/100 \t train loss 0.10984445363283157 \t val loss 0.13577747344970703\n",
      "\t partial train loss : 0.109461\n",
      "\n",
      " EPOCH 79/100 \t train loss 0.10946051776409149 \t val loss 0.13587075471878052\n",
      "\t partial train loss : 0.109395\n",
      "\n",
      " EPOCH 80/100 \t train loss 0.1093946173787117 \t val loss 0.1352851837873459\n",
      "\t partial train loss : 0.109097\n",
      "\n",
      " EPOCH 81/100 \t train loss 0.10909699648618698 \t val loss 0.1353932023048401\n",
      "\t partial train loss : 0.108805\n",
      "\n",
      " EPOCH 82/100 \t train loss 0.10880500823259354 \t val loss 0.13510312139987946\n",
      "\t partial train loss : 0.108656\n",
      "\n",
      " EPOCH 83/100 \t train loss 0.10865578800439835 \t val loss 0.13535036146640778\n",
      "\t partial train loss : 0.108072\n",
      "\n",
      " EPOCH 84/100 \t train loss 0.10807203501462936 \t val loss 0.1348673403263092\n",
      "\t partial train loss : 0.107975\n",
      "\n",
      " EPOCH 85/100 \t train loss 0.10797496140003204 \t val loss 0.13469313085079193\n",
      "\t partial train loss : 0.107762\n",
      "\n",
      " EPOCH 86/100 \t train loss 0.10776153206825256 \t val loss 0.1345970779657364\n",
      "\t partial train loss : 0.107617\n",
      "\n",
      " EPOCH 87/100 \t train loss 0.10761714726686478 \t val loss 0.1345052868127823\n",
      "\t partial train loss : 0.107283\n",
      "\n",
      " EPOCH 88/100 \t train loss 0.10728337615728378 \t val loss 0.13432501256465912\n",
      "\t partial train loss : 0.107019\n",
      "\n",
      " EPOCH 89/100 \t train loss 0.1070185974240303 \t val loss 0.13426104187965393\n",
      "\t partial train loss : 0.107091\n",
      "\n",
      " EPOCH 90/100 \t train loss 0.10709084570407867 \t val loss 0.13414664566516876\n",
      "\t partial train loss : 0.106685\n",
      "\n",
      " EPOCH 91/100 \t train loss 0.10668542236089706 \t val loss 0.13405367732048035\n",
      "\t partial train loss : 0.106656\n",
      "\n",
      " EPOCH 92/100 \t train loss 0.10665567219257355 \t val loss 0.1340252161026001\n",
      "\t partial train loss : 0.106235\n",
      "\n",
      " EPOCH 93/100 \t train loss 0.10623490065336227 \t val loss 0.13384735584259033\n",
      "\t partial train loss : 0.106050\n",
      "\n",
      " EPOCH 94/100 \t train loss 0.1060502901673317 \t val loss 0.13373470306396484\n",
      "\t partial train loss : 0.106215\n",
      "\n",
      " EPOCH 95/100 \t train loss 0.1062152236700058 \t val loss 0.13367696106433868\n",
      "\t partial train loss : 0.105550\n",
      "\n",
      " EPOCH 96/100 \t train loss 0.10554993152618408 \t val loss 0.13361121714115143\n",
      "\t partial train loss : 0.105463\n",
      "\n",
      " EPOCH 97/100 \t train loss 0.10546337813138962 \t val loss 0.13362915813922882\n",
      "\t partial train loss : 0.105378\n",
      "\n",
      " EPOCH 98/100 \t train loss 0.10537837445735931 \t val loss 0.13374210894107819\n",
      "\t partial train loss : 0.105223\n",
      "\n",
      " EPOCH 99/100 \t train loss 0.10522273927927017 \t val loss 0.13364654779434204\n",
      "\t partial train loss : 0.104852\n",
      "\n",
      " EPOCH 100/100 \t train loss 0.10485193133354187 \t val loss 0.13354690372943878\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "   train_loss =train_epoch(encoder,decoder, train_loader,loss_fn,optim)\n",
    "   val_loss = test_epoch(encoder,decoder,test_loader,loss_fn)\n",
    "   print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "   diz_loss['train_loss'].append(train_loss)\n",
    "   diz_loss['val_loss'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [0.3501894, 0.24923097, 0.19237554, 0.16729727, 0.15515763, 0.14901991, 0.14484873, 0.14263119, 0.14065386, 0.13942668, 0.13807847, 0.13689698, 0.13610095, 0.13507926, 0.13374749, 0.13326631, 0.13281533, 0.13134432, 0.13129932, 0.13037688, 0.12977347, 0.12902755, 0.12868206, 0.12797868, 0.12745878, 0.12727575, 0.1267216, 0.12579843, 0.12549724, 0.12493442, 0.12474664, 0.12435983, 0.12361403, 0.12316246, 0.123007044, 0.122422874, 0.12220512, 0.1217172, 0.12139263, 0.12086224, 0.12062247, 0.12008834, 0.11985132, 0.11952729, 0.119145274, 0.118977256, 0.11838817, 0.11807795, 0.117576145, 0.1172686, 0.11729575, 0.11736542, 0.116540834, 0.11619243, 0.11603162, 0.11554, 0.11527678, 0.1149944, 0.1147902, 0.114473626, 0.1143151, 0.113895185, 0.11365462, 0.11346311, 0.113219015, 0.1127968, 0.112504296, 0.11246866, 0.112210035, 0.11194058, 0.111816905, 0.11157965, 0.111099735, 0.11091486, 0.110569276, 0.11030422, 0.11024826, 0.10984445, 0.10946052, 0.10939462, 0.109097, 0.10880501, 0.10865579, 0.108072035, 0.10797496, 0.10776153, 0.10761715, 0.107283376, 0.1070186, 0.107090846, 0.10668542, 0.10665567, 0.1062349, 0.10605029, 0.10621522, 0.10554993, 0.10546338, 0.105378374, 0.10522274, 0.10485193], 'val_loss': [tensor(0.1665), tensor(0.1671), tensor(0.1676), tensor(0.1680), tensor(0.1685), tensor(0.1692), tensor(0.1699), tensor(0.1704), tensor(0.1715), tensor(0.1717), tensor(0.1723), tensor(0.1729), tensor(0.1722), tensor(0.1719), tensor(0.1703), tensor(0.1702), tensor(0.1686), tensor(0.1675), tensor(0.1662), tensor(0.1656), tensor(0.1635), tensor(0.1630), tensor(0.1627), tensor(0.1607), tensor(0.1605), tensor(0.1596), tensor(0.1586), tensor(0.1579), tensor(0.1575), tensor(0.1570), tensor(0.1565), tensor(0.1562), tensor(0.1553), tensor(0.1550), tensor(0.1548), tensor(0.1535), tensor(0.1536), tensor(0.1532), tensor(0.1533), tensor(0.1516), tensor(0.1510), tensor(0.1506), tensor(0.1504), tensor(0.1492), tensor(0.1494), tensor(0.1488), tensor(0.1475), tensor(0.1473), tensor(0.1464), tensor(0.1465), tensor(0.1446), tensor(0.1447), tensor(0.1441), tensor(0.1435), tensor(0.1432), tensor(0.1423), tensor(0.1419), tensor(0.1418), tensor(0.1413), tensor(0.1408), tensor(0.1404), tensor(0.1400), tensor(0.1396), tensor(0.1392), tensor(0.1388), tensor(0.1389), tensor(0.1384), tensor(0.1379), tensor(0.1382), tensor(0.1375), tensor(0.1371), tensor(0.1372), tensor(0.1368), tensor(0.1365), tensor(0.1366), tensor(0.1363), tensor(0.1360), tensor(0.1358), tensor(0.1359), tensor(0.1353), tensor(0.1354), tensor(0.1351), tensor(0.1354), tensor(0.1349), tensor(0.1347), tensor(0.1346), tensor(0.1345), tensor(0.1343), tensor(0.1343), tensor(0.1341), tensor(0.1341), tensor(0.1340), tensor(0.1338), tensor(0.1337), tensor(0.1337), tensor(0.1336), tensor(0.1336), tensor(0.1337), tensor(0.1336), tensor(0.1335)]}\n",
      "range(0, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14105244670>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "print(diz_loss)\n",
    "x_ax = range(len(diz_loss['train_loss']))\n",
    "print(x_ax)\n",
    "x_tcs = np.arange(0.1, 0.5, 0.01)\n",
    "# ax.set_ylim([0.1, 0.4])\n",
    "plt.yticks(x_tcs)\n",
    "plt.plot(x_ax, diz_loss['train_loss'])\n",
    "plt.plot(x_ax, diz_loss['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD model from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def MMD(x, y):\n",
    "    '''\n",
    "    Using gaussian kernel for MMD\n",
    "    '''\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "    print(xx, xx.diag(),xx.diag().unsqueeze(0), rx)\n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "    \n",
    "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device))\n",
    "    \n",
    "    # applying kernel method\n",
    "    sigmas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100, 1e3, 1e4, 1e5, 1e6]\n",
    "    for sigma in sigmas:\n",
    "        XX += torch.exp(-0.5*dxx/sigma)\n",
    "        YY += torch.exp(-0.5*dyy/sigma)\n",
    "        XY += torch.exp(-0.5*dxy/sigma)\n",
    "\n",
    "    return torch.mean(XX + YY - 2. * XY)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a52b259244b164c4849f7abc64962394aa861a8ee3a9faf3dff56f5ce380f3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('scanpy_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
